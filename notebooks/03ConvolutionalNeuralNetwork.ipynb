{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/k5yi/econ2005/blob/master/notebooks/03ConvolutionalNeuralNetwork.ipynb\">\n",
    "  <img src=\"https://img.shields.io/badge/%EC%84%9C%EA%B0%95%EA%B2%BD%EC%A0%9C-3%20Convolutional%20Neural%20Network -crimson?labelColor=navy&logo=googlecolab&logoClolor=crimson\" align='left'/>\n",
    "</a> <br><br>\n",
    "\n",
    "\n",
    "## Deep Neural Network 의 특징\n",
    "\n",
    "- 자료의 noise에 강하며 missing value가 있어도 추정이 가능하다.\n",
    "- Time series data나 text 같은 sequential 자료나 image 같은 다차원 자료인 경우 그 구조를 유지하기 어렵다.\n",
    "- 학습속도, 수렴 여부와 관련해 gradient의 값이 안정적이지 않은 경우가 많다.\n",
    "- 모형이 '깊어질수록' vanishing/exploding gradient 문제로 weight의 update가 힘들어진다.\n",
    "- 시간과 공간에 대한 정보를 제대로 활용하지 못한다.\n",
    "\n",
    "\n",
    "- Convolution Neural Network (CNN)\n",
    " - image의 경우 경계와 같은 국지적 정보에 집중하여 대상을 식별한다.\n",
    " - 정보의 위치에 구애를 받지 않지만 위치를 정확하게 알려주진 않는다.\n",
    " - 한번에 하나의 대상만을 구분할 수 있다.\n",
    " - 공간과 시간을 같은 모형으로 분석할 수 있다.\n",
    " \n",
    "\n",
    "- Recurrent Neural Network (RNN)\n",
    " - 입력 sequence가 길어지면 ANN과 마찬가지로 gradient가 안정적이지 않을 수 있다.\n",
    " - Long Short-Term Memory (LSTM), Attention, Tranformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image 와 sequential data\n",
    "\n",
    "- 2012년 [Krizhevsky, Sutskever, Hilton](https://proceedings.neurips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)이 CNN을 이용하여 ImageNet 자료의 예측오차를 이전 분류모형들보다 크게 감소시킨 이후 CNN이 널리 사용되기 시작하였다.\n",
    "\n",
    "\n",
    "\n",
    "- CNN의 특징은 존재와 공간구조이다.\n",
    "- 공간이나 시간에 대한 정보를 표시하거나 해석할 때는 왼쪽에서 오른쪽, 혹은 위에서 아래로 진행한다. 공간과 시간에 대한 자료들은 유사한 분석 순서를 갖는다.\n",
    "\n",
    "- 시계열 자료는 공간으로 mapping이 가능하므로 image와 같은 모형으로 분석할 수 있다. CNN의 활용범위가 넓어지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Neural Network, CNN, ConvNets\n",
    "\n",
    "\n",
    "- 1958, 1959년 Hubel과 Wiesel은 고양이를 이용한 시각인식 실험으로 많은 neuron들이 국지적인 자극에 반응하며 일부 neuron들은 광역적인 자극에 반응한다는 밝혀내었다.\n",
    "또한 neuron들의 반응은 수평선이나 수직선, 사선과 같은 극히 단순한 형태에 특화되어 있으며 이런 단순한 형태로 구성된 복잡한 pattern을 인식하는 neuron들이 별도로 존재한다는 것을 보였다.\n",
    "이 발견은 Convolutional Neural Network의 발전에 큰 영향을 미쳤다.\n",
    "\n",
    "\n",
    "<img src='https://github.com/k5yi/econ2005/blob/master/images/imagenet.png?raw=True'>\n",
    "출처: Krizhevsky, Sutskever, Hilton (2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CNN은 기능적으로 DNN과 상당히 유사하지만 다음과 같은 특징으로 인해 image에 특화되어 발전되어 왔다.\n",
    "\n",
    "1. 공간상의 배열에 대한 정보를 유지하고\n",
    "2. 추정 모수의 수는 입력물의 크기와 무관하게 결정되므로 그 수를 극적으로 감소시킬 수 있다.\n",
    "\n",
    "\n",
    "- 과거 CNN는 대부분 image와 관련된 분석에 사용하였지만, 최근 parallel computing과 transfer learning이 가능하다는 장점으로 인해 자연어 처리 등으로 응용분야가 넓어지고 있다.\n",
    "\n",
    "\n",
    "> “\\[m\\]ost of this progress is not just the result of more powerful hardware, larger datasets and bigger models, but mainly a consequence of new ideas, algorithms and improved network architectures.” (Szegedy et al, 2014) [재인용](https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d)\n",
    "\n",
    "\n",
    "- CNN의 큰 단점으로 지적되는 것 중 하나는 훈련에 필요한 자료의 양이다.\n",
    "\n",
    "\n",
    "- Geometric transformation, \n",
    " - flipping, color space, cropping, rotation, translation, GAN based augmentation\n",
    "- photometric transformation \n",
    " - kernel filters, mixing images, random erasing, \n",
    "- Data Augmentations based on Deep Learning\n",
    " - Feature space augmentation, adversarial training, GAN-based Data Augmentation, Neural Style Transfer, Meta learning Data Augmentations, Neural augmentation, Smart Augmentation, AutoAugment\n",
    " \n",
    "[A survey on Image Data Augmentation for Deep Learning, Shorten and Khoshgoftaar, 2019, Journal of Big Data](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "\n",
    "### 기본 아이디어\n",
    "\n",
    "<img src=\"https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/10/3-7-768x191.png\">\n",
    "\n",
    "1. 첫 이미지를 image reference로 삼아 두 번째 이미지를 평가한다면 겹치지 않는 부분이 대부분이다.\n",
    "2. digit image의 예와 같이 위치가 바뀐다면 거의 겹치지 않을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 고양이의 인지방법에 착안하여 부분적인 특징에 집중한다.\n",
    "- 특정형태에 반응하는 **kernel** 혹은 **filters** 를 적용하여 (위치에 관계없이) 그림에 상응하는 형태가 존재하는지 파악한다.\n",
    "\n",
    "\n",
    "<img src=\"kernels.png\" width=600>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/10/3-8-768x259.png\">\n",
    "\n",
    "- 적당한 크기의 kernel로 부분과 부분을 비교하면 왼쪽에 있는 kernel 상의 이미지 전부가 오른쪽 이미지에 존재한다. 즉 전체 이미지가 '일치'한다고 판단한다.\n",
    "\n",
    "- CNN의 특징은 feature가 어디에 위치하는가가 아니라 존재하느냐 하지 않느냐 이다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network의 특징\n",
    "\n",
    "\n",
    "- CIFAR-10의 경우 각 image는 $32\\times32\\times3=3,072$개의 정보로 이루어져 있다. 이를 fully connected NN에 적용하면 첫 layer에서는 각 neuron당 3,072 개의 weight parameter를 학습해야 하므로, layer가 넓어지고 깊어짐에 따라 추정모수의 수가 기하급수적으로 증가한다.\n",
    "\n",
    "\n",
    "- ConvNets에서는 각 unit을 국지적으로 연결하거나 모수를 공유하여 이러한 문제를 해결한다.\n",
    "\n",
    "\n",
    "### (Local) Receptive Field\n",
    "\n",
    "- DNN과 같이 전체 입력을 한번에 처리하면 공간(다차원 공간 상의 배열)에 대한 정보를 유지하기 어려워 국지적인 특징을 잡아내기 어렵다.\n",
    "\n",
    "- Neuron 사이의 직접적인 연결은 (width, height) 공간의 일부 \"지역\"으로 국한하며 이 범위를 receptive field라고 한다.\n",
    "\n",
    "- ConvNets는 lower level layer에서 선이나 모서리와 같은 low-level feature를 학습하고 higher layer에서는 lower layer에서 학습한 내용을 축적하여 점차 큰 범위를 고려해 가면서 shape과 같은 추상적인 특징들을 학습한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Sharing\n",
    "\n",
    "\n",
    "- ConvNets의 neuron은 receptive field의 pixel 값과 kernel의 convolution을 처리하는 단위이다. 따라서 각 neuron에는 receptive field의 pixel수에 channel 수를 곱한 것에 해당하는 모수가 포함된다. Image 자료로 일반화된 ConvNets 모형을 학습하면 추정할 모수의 수가 감당하기 어려울만큼 많아지게 된다.\n",
    "\n",
    "\n",
    "- 예를 들어 Shape이 $(8, 8, 3)$인 digit image의 receptive field에 $(2, 2, 1)$ 크기의 kernel을 적용한다면 결과물은 (7,7,3)인 feature map이 된다.  <br>\n",
    "따라서 각 receptive field에 다른 kernel을 적용하면 각 filter당 $(2\\times 2 + 1) \\times (7\\times 7\\times 3)$개의 weight를 학습해야 한다. <br>\n",
    "Image의 크기와 filter의 수가 증가하면서 추정해야할 모수의 수는 기하급수적으로 증가하게 된다.\n",
    "\n",
    "\n",
    "- Transition invariance <br>\n",
    "ConvNets의 본질은 특정 형태를 찾는 것이므로 그 형태가 image의 어디에 위치하던지 동일한 filter로 파악이 가능하다면 다른 위치의 pixel들에 굳이 다른 filter를 적용할 필요가 없다. 수학적으론 모든 대상 pixel에 동일한 vector를 곱하는 것을 의미한다.\n",
    "\n",
    "\n",
    "- Parameter sharing <br>\n",
    "Transition invariance에 근거하여 width와 height축에 적용하는 filter의 weight와 bias가 각 feature map(혹은 depth slice)에서 동일하다고 가정한다. 하나의 feature map에는 $(k\\times k)\\times \\text{depth} + 1$ 만큼의 모수가 필요하게 된다.\n",
    "\n",
    "\n",
    "- Depth가 1인 digit image에서 parameter sharing을 가정하면 크기가 (2,2)인 kernel을 이용하여 하나의 feature map 생성하는데 필요한 모수의 수는 5개가 된다. Convolutional layer에서 결정해야할 모수의 수는 width나 height와는 무관하게 결정된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pooling/spatial sub-sampling\n",
    "\n",
    "- 분석의 목적이 특정 형태의 위치가 아니라 존재 여부라면 위치에 대한 정보를 굳이 유지할 필요가 없다. 불필요한 추가 정보는 대부분 overfitting 문제를 일으킨다.\n",
    "\n",
    "- Subsampling은 가급적 위치에 대한 정보를 희석시켜 training dataset에 대한 의존을 줄이고 상대적인 위치에 집중할 수 있도록 해준다.\n",
    "\n",
    "- 위치 정보가 중요한 detection (고양이가 어디에 있는지)이나 segmentation (구분한 형상이 고양이인지) 문제에서는 더 큰 영역을 살피기 위해 dilated convolution을 사용하거나 위치 정보를 온전히 유지하기 위해 $1\\times 1$ kernel을 사용하기도 한다.\n",
    "\n",
    "\n",
    "#### Pooling \n",
    "\n",
    "- feature map의 가장 눈에 띄는 부분만 취하기도 하고 (max pooling) 해당 영역 값들의 평균을 사용하기도 한다 (average pooling). \n",
    "\n",
    "\n",
    "#### Striding \n",
    "- Convolutional layer에서 feature map을 생성할 때 filter의 이동간격을 조정하여 출력의 크기를 조절하기도 하고 pooling 단계에서 이동간격을 조정하여 subsampling에 사용한다. kernel의 크기가 2일 때 이동간격인 stride가 2이면 feature map을 생성할 때 각 pixel의 정보는 한번 씩만 사용된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ConvNets의 구조\n",
    "\n",
    "- CNN은 입력자료를 (width, height, depth)로 구분한 3차원으로 정리하여 사용한다.<br>\n",
    "궁극적으로 $(1,1,C)$로 만들어 classification에 사용한다.<br>\n",
    "다음은 분류에서 많이 사용하는 기본적인 CNN 모형의 구조이다.\n",
    "\n",
    "1. Input layer (w, h, d)\n",
    "2. Convolutional layer (w', h', n_filters)\n",
    "3. Smoothing layer (w', h', n_filters) - activation functions, no parameters\n",
    "4. Pool layer (w\", h\", n_filters) - no parameters\n",
    "5. (Fully-connected) dense layer (1,1,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "- ConvNets의 input은 shape이 (None, width, height, depth)인 4차원 tensor로 준비한다.\n",
    "\n",
    "\n",
    "- 각 receptive field와 chennel에는 동일한 kernel을 적용하여 최종 feature map을 구성한다. 따라서 Output의 depth, 즉 feature map의 수는 입력 depth 혹은 channel 수와 무관하게 결정된다.\n",
    "\n",
    "\n",
    "- Dense layer와 마찬가지로 첫번째 layer에 `input_shape`을 지정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutioal layers\n",
    "\n",
    "- ConvNets에서 가장 중요한 layer로 대부분의 연산은 convolutional layer에서 이루어진다.\n",
    "\n",
    "#### [filters](https://ai.stanford.edu/~syyeung/cvweb/tutorial1.html#:~:text=Two%20commonly%20implemented%20filters%20are,image%20with%20sharp%20features%20removed)\n",
    "\n",
    "- Convolutoinal layer는 dense layer 같이 전체 input에 대해 fully connected network을 구성하는 것이 아니라 input의 receptive field에 대해 dense network을 반복적으로 적용한 것이다.\n",
    "- Receptive field에 적용하는 weights를 [kernels, filters](https://en.wikipedia.org/wiki/Kernel_(image_processing)) 혹은 convolution kernels, convolution matrix 라고 한다.\n",
    "- Filter는 receptive field 단위로 정보를 처리하여 국지적 정보에 집중할 수 있도록 한다.\n",
    "\n",
    "\n",
    "- 선택한 $k_0 \\times k_1$ 크기의 kernel을 이용하여 $k_0 \\times k_1$의 receptive field와의 가중합을 구한다. 이러한 연산방법을 합성곱 convolution이라고 한다. Filter의 depth는 입력자료의 depth 혹은 \"feature의 수\"와 동일게 구성하므로 생략한다.\n",
    "\n",
    "```python\n",
    "tf.keras.layers.Conv2D(filters=6, kernel_size=5, input_shape=(8,8,3))\n",
    "```\n",
    "\n",
    "- `filters`는 filter의 수 혹은 feature map의 수이고 output의 depth를 결정한다. `kernel_size`로 kernel의 크기를 지정하며 width와 height가 같을 필요는 없다.\n",
    "\n",
    "\n",
    "- Filter를 이용해 계산한 convolution을 정리한 tensor를 feature map 혹은 activation map이라 한다.\n",
    "\n",
    "\n",
    "\n",
    "- Layer의 trainable parameter의 수는 각 feature map 마다 $(k_0\\times k_1 \\times d) + 1$으로 전적으로 kernel의 크기와 filter의 수에 의해 결정된다. 여기서 $d$는 입력의 마지막 축인 depth의 크기이다.\n",
    "\n",
    "\n",
    "- kernel vs. filter\n",
    "  - kernel은 input과 곱해지는 weight 행렬\n",
    "  - filter는 여러 kernel의 묶음으로 kernel보다 하나의 차원을 더 갖는다.\n",
    "  - 2D의 경우 kernel은 $k \\times k$, filter는 $k \\times k \\times \\text{depth}$\n",
    "  - 보통 구분하지 않고 사용하는 경우가 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input_shape = (4, 4, 2)이고 kernel = 3 이라면 첫번째 feature map은 다음과 같이 계산한다.\n",
    "- 입력자료가 여러 channel을 갖는다면 필터는 각 channel에 독립적인 kernel을 적용하여 feature map을 생성한 후, 모든 channel의 feature map을 합산하여 최종 feature map을 생성한다. 따라서 입력 자료의 channel수와 무관하게 filter 별로 하나의 feature map이 만들어진다.\n",
    "\n",
    "\n",
    "```python\n",
    "feature_map[0,0,0] = np.sum(x[:3,:3,:]*W0) + b0\n",
    "feature_map[0,1,0] = np.sum(x[:3,1:,:]*W0) + b0\n",
    "feature_map[1,0,0] = np.sum(x[1:,:3,:]*W0) + b0\n",
    "feature_map[1,1,0] = np.sum(x[1:,1:,:]*W0) + b0\n",
    "\n",
    "feature_map[0,0,1] = np.sum(x[:3,:3,:]*W1) + b1\n",
    "feature_map[0,1,1] = np.sum(x[:3,1:,:]*W1) + b1\n",
    "feature_map[1,0,1] = np.sum(x[1:,:3,:]*W1) + b1\n",
    "feature_map[1,1,1] = np.sum(x[1:,1:,:]*W1) + b1\n",
    "\n",
    "feature_map = np.sum(feature_map, axis=-1)\n",
    "```\n",
    "\n",
    "- channel의 합이 의미가 없다면, channel 별로 feature map을 만든 후 마지막 layer에서 결과를 합쳐 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 2, 126, 1)         1153      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,153\n",
      "Trainable params: 1,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, Conv3D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=1, kernel_size=(3,3), strides=1, input_shape = (4, 128, 128)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel of feature map, depth 0 \n",
      " [[[-0.06411681]\n",
      "  [ 0.15077472]\n",
      "  [-0.07951865]]\n",
      "\n",
      " [[-0.19798239]\n",
      "  [ 0.2244581 ]\n",
      "  [-0.27841812]]\n",
      "\n",
      " [[-0.2552897 ]\n",
      "  [ 0.24131185]\n",
      "  [ 0.05768529]]]\n",
      "\n",
      "kernel of feature map, depth 1 \n",
      " [[[ 0.10630333]\n",
      "  [-0.27025616]\n",
      "  [ 0.18888742]]\n",
      "\n",
      " [[-0.36210284]\n",
      "  [ 0.33423513]\n",
      "  [-0.35434562]]\n",
      "\n",
      " [[-0.38979915]\n",
      "  [ 0.24658233]\n",
      "  [ 0.1945976 ]]]\n",
      "\n",
      "kernel of feature map, depth 2 \n",
      " [[[ 0.39900303]\n",
      "  [-0.18263222]\n",
      "  [-0.2148967 ]]\n",
      "\n",
      " [[ 0.3323859 ]\n",
      "  [ 0.40444118]\n",
      "  [-0.2530946 ]]\n",
      "\n",
      " [[ 0.1277265 ]\n",
      "  [-0.2787545 ]\n",
      "  [ 0.16274202]]]\n",
      "\n",
      "bias\n",
      " [0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f'kernel of feature map, depth {i} \\n', model.get_weights()[0][i])\n",
    "    print()\n",
    "    \n",
    "print('bias\\n', model.get_weights()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN layers의 차원과 layer의 선택\n",
    "\n",
    "- `tf.keras.layers.Conv1D`, `Conv2D`, `Conv3D`에서 차원의 구분은 kernel을 움직이는 공간의 차원을 기준으로 한다. \n",
    "- 통상적인 2차원 color image의 경우 색을 표현하기 위해 RGB 자료가 필요하므로 자료구조는 3차원이다. <br>\n",
    "하지만 각 channel에 독립적인 kernel을 적용하므로 kernel이 움직이는 공간은 2차원이다.\n",
    "\n",
    "\n",
    "- 1D는 -2번째 축을 따라 kenel을 움직이면서 convolution이 이루어지고, 2D는 (-3,-2)번째 축, 3D는 축 (-4, -3, -2)을 따라 각 depth별로 filter를 적용한다. 즉 마지막 축의 원소마다 하나의 feature map을 만들어 모두 더해 최종 feature map을 완성한다.\n",
    "\n",
    "\n",
    "- Conv**N**D에서 N=1,2,3이라고 하면 receptive field는 `input_shape[-(N+1):-1]`에 의해 결정된다.\n",
    "- Output shape에서 `[-(N+1):-1]`사이의 축의 길이는 kernel과 다음에 설명할 stride, padding에 따라 달라지며 `[-1]`축은 filter의 갯수 혹은 최종 feature map의 이전 단계에서 생성하는 feature map의 수가 결정한다.\n",
    "- 어떤 경우에도 batch_size와 마지막 depth 축 사이의 공간으로 kernel이 움직인다고 생각하자. 예를 들어 Con1D의 입력자료가 3차원 (batch_size, timesteps, features) 보다 커지면 batch_size가 여러 개의 차원으로 구성된다고 보면 이해하기 쉽다.\n",
    "\n",
    "\n",
    "- Output의 마지막 축은 feature map의 갯수, 즉 filter의 갯수이므로 convolution layer의 입력과 출력 차원은 동일하다.\n",
    "- 4D 자료를 Conv1D나 Conv2D layer를 이용해 처리할 수 있지만 filter 크기에 대한 제약이 다르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 다음 예는 stride가 1, padding이 없을 경우에 해당한다.\n",
    "\n",
    "\n",
    "- Conv1D<br>\n",
    "(None, timesteps, features)로 해석하면 기억하기 쉽다. kernel은 크기가 (k,)이고 timestep 축으로 만 움직이므로 feature map의 크기는 (None, timesteps-k+1, n_filters)가 된다. <br>\n",
    "1D CNN의 입출력은 보통 2차원이며 timesteps을 이용한 시계열자료 분석에 주로 이용된다.\n",
    "\n",
    "\n",
    "- Conv2D<br>\n",
    "input_shape이 (None, width, height, depth)일 때 filter의 shape은 (k, k)이므로 filter가 움직일 수 있는 방향은 width와 height 두 축 뿐이므로 Conv2D를 사용한다. Conv2D의 ouput shape은 (None, width-k+1, height-k+1, n_filters)과 된다.<br>\n",
    "2D CNN의 입출력자료는 기본적으로 3개의 축으로 표시할 수 있으며 image 자료 분석에 주로 사용한다.\n",
    "\n",
    "\n",
    "- Conv3D<br>\n",
    "이 경우 전형적인 input_shape은 (None, width, height, depth, n_features)이고 width, height, depth 축 방향으로 kernel을 이동시킨다.<br>\n",
    "3D CNN은 MRI나 CT scan과 같은 4차원 입체 image에 많이 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "tf.keras.layers.Conv1D(\n",
    "    filters, kernel_size, strides=1, padding='valid',\n",
    "    data_format='channels_last', dilation_rate=1, groups=1,\n",
    "    activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros', kernel_regularizer=None,\n",
    "    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
    "    bias_constraint=None, **kwargs\n",
    ")\n",
    "\n",
    "tf.keras.layers.Conv2D(\n",
    "    filters, kernel_size, strides=(1, 1), padding='valid',\n",
    "    data_format=None, dilation_rate=(1, 1), groups=1, activation=None,\n",
    "    use_bias=True, kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros', kernel_regularizer=None,\n",
    "    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
    "    bias_constraint=None, **kwargs\n",
    ")\n",
    "\n",
    "tf.keras.layers.Conv3D(\n",
    "    filters, kernel_size, strides=(1, 1, 1), padding='valid',\n",
    "    data_format=None, dilation_rate=(1, 1, 1), groups=1, activation=None,\n",
    "    use_bias=True, kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros', kernel_regularizer=None,\n",
    "    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
    "    bias_constraint=None, **kwargs\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 7, 8, 32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# With extended batch shape [4, 7] (e.g. weather data where batch\n",
    "# dimensions correspond to spatial location and the third dimension\n",
    "# corresponds to time.)\n",
    "\n",
    "input_shape = (4, 7, 10, 128)\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.keras.layers.Conv1D(filters=32, kernel_size=3, \n",
    "                           activation='relu', input_shape=input_shape[2:])(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 24, 24, 2)\n"
     ]
    }
   ],
   "source": [
    "# With `dilation_rate` as 2.\n",
    "\n",
    "input_shape = (4, 28, 28, 3)\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.keras.layers.Conv2D(\n",
    "2, 3, activation='relu', dilation_rate=2, input_shape=input_shape[1:])(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 26, 26, 26, 2)\n"
     ]
    }
   ],
   "source": [
    "# The inputs are 28x28x28 volumes with a single channel, and the\n",
    "# batch size is 4\n",
    "\n",
    "input_shape =(4, 28, 28, 28, 1)\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.keras.layers.Conv3D(filters=2, kernel_size=3, \n",
    "                           activation='relu', input_shape=input_shape[1:])(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $1\\times 1$ convolution\n",
    "\n",
    "- 위치 정보를 온전히 보존하기 위해 사용한다. kernel size가 1이지만 유효한 filter의 크기는 $(1\\times 1\\times \\text{depth})$에 해당하므로 3차원 자료에 대해선 특별한 용도로 사용한다.\n",
    "\n",
    "\n",
    "#### Dilated convolution\n",
    "\n",
    "- kernel의 행과 열 사이에 weight가 0인 행과 열을 삽입하여 parameter의 수를 증가시키지 않고 receptive field를 크게할 수 있다. \n",
    "- `dilation_rate`는 kernel의 한 행(열)과 다음 행(열)사이의 거리로 kernel의 크기는 $ \\text{dilation_rate} \\times (k-1) + 1$이며, `dilation_rate=1`은 kernel을 변화시키지 않는다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strides\n",
    "\n",
    "- Stride는 convolution 계산에서 kernel의 이동거리를 의미하며 receptive field의 크기가 온전히 유지될 때까지만 진행하므로 stride가 1보다 크다면 마지막 일부 자료를 포함하지 않을 수 있다. <br>\n",
    "\n",
    "\n",
    "- Striding의 주 목적은 해상도를 떨어뜨리고(downsampling) receptive field의 중복을 피해 계산속도를 향상시키는 것이다. Striding은 parameter sharing의 한 방법으로 모형의 실질적인 performance에는 별다른 영향을 주지 않다는 주장도 있다. [참고](https://www.arxiv-vanity.com/papers/1712.02502/)\n",
    "\n",
    "\n",
    "- Stride가 1이라면 $w\\times h$ 이미지는 $(w-k+1)\\times (h-k+1)$ 크기의 feature map으로 변환된다.<br>\n",
    "일반적으로 input shape이 (w, h, d)이고 kerel이 k, stride가 s라면 feature map의 크기는 다음과 같이 결정된다.\n",
    "\n",
    "$$ (w^o, h^o, d^o) = \\left(\\frac{w-k}{s}+1, \\frac{w-k}{s}+1, 1 \\right) $$\n",
    "\n",
    "- 필요하면 padding 처리로 원본과 동일한 크기의 feature map을 만들 수 있다.\n",
    "- dilated filter를 사용하려면 stride=1로 지정해야 한다. (Tensorflow 2.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 8, 4, 1)           16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16\n",
      "Trainable params: 16\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential(name='strides')\n",
    "model.add(tf.keras.layers.Conv2D(1, (3,5), strides=(1,2), padding='same', input_shape=(8, 8, 1)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding\n",
    "\n",
    "- 사용하는 kernel에 따라 경계에 있는 정보를 feature map에 제대로 반영하지 못할 수 있다.<br>\n",
    "Padding을 추가하지 않으면 (padding=\"valid\") 제일 첫 원소부터 차례로 filter를 적용하면서 짝이 맞지않는 원소들은 분석에서 고려하지 않는다. \n",
    "\n",
    "\n",
    "- 이러한 문제를 해결하기 위해 image의 경계에 zero pad를 추가하여 kernel를 적용한다. Padding에 사용하는 숫자가 0이므로 convolution 값에는 영향을 미치지 않는다.\n",
    "\n",
    "- 일반적으로 input shape이 (w, h, d)이고 kerel이 k, stride가 s, zero padding의 크기가 p라면 feature map의 크기는 다음과 같이 결정된다.\n",
    "\n",
    "$$ (W^o, h^o, d^o) = \\left(\\frac{w-k+2p}{s}+1, \\frac{w-k+2p}{s}+1, 1 \\right) $$\n",
    "\n",
    "\n",
    "- `padding` keyword에서 \"valid\"는 pad를 추가하지 않고 kernel의 크기에 맞는 유효한 자료만 고려하며, \"same\"은 위아래, 좌우에 동일한 수의 padding을 적용한다. padding=\"same\"이고 strides=1 이라면 output은 input과 동일한 shape을 갖는다.\n",
    "\n",
    "\n",
    "- 예: input=np.array($[[1,2,3,4,5,6,7,8]]$), kernel = 4, stride = 3<br>\n",
    "padding=\"valid\": (1,2,3,4), (4,5,6,7), **(7,8)-dropped**<br>\n",
    "padding=\"same\": (**0**,1,2,3), (3,4,5,6), (6,7,8,**0**)\n",
    "\n",
    "\n",
    "- 필요에 따라 `ZeroPadding1D, ZeroPadding2D, ZeroPadding3D`로 pad의 수를 임의로 정할 수 있다. 2D의 경우 정수나, tuple, 혹은 tuple의 tuple로 크기를 지정한다. 순서는 축의 순서를 따른다.<br><br>\n",
    "(symmetric_height_pad, symmetric_width_pad)<br>\n",
    "((top_pad, bottom_pad), (left_pad, right_pad))\n",
    "\n",
    "\n",
    "- Conv1D layer는 padding option으로 'causal'이 있다. `output[t]`이 `input[t+1:]`에 의존하지 않을 경우 input의 앞부분에 k-1개의 zero padding을 추가하여 불완전한 자료로 첫 $k-1$개의 값을 예측한다.\n",
    "시작이 정해지지 않는 시계열자료에는 별다른 의미가 없지만 노래나 문장같이 시작이 정해져 있는 sequence의 경우 시작과 진행을 구분할 수 있게 해준다.\n",
    "- causal padding을 사용하면 input과 output의 길이가 같다진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 8, 8, 1)           10        \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 6, 6, 1)           10        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential(name='paddings')\n",
    "model.add(tf.keras.layers.Conv2D(1, (3,3), padding='same', input_shape=(8, 8, 1)))\n",
    "model.add(tf.keras.layers.Conv2D(1, (3,3), padding='valid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AR(k) series 예측을 위한 Neural Network 모형\n",
    "\n",
    " - DNN 모형: `Dense(1, input_shape=(T-k,k))`\n",
    "\n",
    " - Conv1D 모형: `Conv1D(1, kernel=k, strides=1, padding='valid', input_shape=(T,1))`\n",
    " \n",
    " - input_shape을 비교해보면 ConvNet 모형의 표본 전처리과정이 더 간단할 수 있다.\n",
    " \n",
    "\n",
    "- [이 blog](https://theblog.github.io/post/convolution-in-autoregressive-neural-networks/)는 시계열자료 분석과 관련된 다양한 모형 기법들을 소개하고 있다. 그림으로 여러 모형들의 특징들을 비교하여 이해하기 쉽다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DNN_AR_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 97, 1)             5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Conv1D_AR_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_1 (Conv1D)           (None, 97, 1)             5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "T=100; k=4\n",
    "\n",
    "model = tf.keras.models.Sequential(name=f'DNN_AR_{k}')\n",
    "model.add(tf.keras.layers.Dense(1, input_shape=(T-k+1,k)))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential(name=f'CNN_AR_{k}')\n",
    "model.add(tf.keras.layers.Conv1D(1, kernel_size=k, strides=1, padding='valid', input_shape=(T,1)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing Layer\n",
    "\n",
    "- 대상의 비선형성을 고려하기 위해 feature map의 각 원소에 비선형변환을 취한다.<br>\n",
    "이 과정을 통해 noise와 variation(변형된 특징)의 영향을 줄일 수 있다.\n",
    "\n",
    "\n",
    "- Tanh나 sigmoid를 사용하기도 하지만 대부분의 분석에서 ReLU를 사용한다. 실증적으로 ReLu의 성과가 다른 변환방법에 비해 더 좋은 것으로 알려져 있다. DNN에서와 마찬가지로 backpropagation과정에서 gradient의 크기로 인한 문제를 완화할 수 있다.\n",
    "\n",
    "- Convolutional layer에서 activation function의 default는 아무것도 적용하지 않는 것이므로 (identity) convlutional layer의 keyword나 별도의 ReLU layer로 적용할 수 있다.\n",
    "\n",
    "```python\n",
    "tf.keras.layers.Conv2D(activation='relu')\n",
    "tf.keras.layers.ReLU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling layer (subsampling)\n",
    "\n",
    "- 모형의 중간중간에 삽입하는 pooling layer는 pooling으로 receptive field의 정보를 하나로 요약하는 기능을 한다.\n",
    " - feature map의 크기를 줄여 학습할 모수의 수를 감소\n",
    " - overfitting 완화\n",
    " - output을 원하는 크기로 변환\n",
    "\n",
    "- 그럼에도 핵심정보의 손실이 크지 않다. (비슷한 위치에서 평균이나 극대값은 잘 변하지 않는다)\n",
    "- Pooling 대신 stride를 크게 하는 것이 위의 목적에 부합하면서 더 좋은 성과를 낼 수도 있다.\n",
    "\n",
    "\n",
    "- pool_size는 filter의 크기를 고려하되 보통은 2x2, 간혹 3x3을 사용하여 연속적으로 적용한다.\n",
    "- 통상 strides의 크기도 pooling_size와 같게 선택하여 kernel이 겹치지 않도록 하는 것이 일반적이다.\n",
    "\n",
    "\n",
    "- 모든 depth에 대해 독립적으로 적용하므로 depth의 크기는 변하지 않는다.\n",
    "- 많이 사용하는 max pooling의 경우 kernel_size=2와 strides=2를 적용하면 전체 activation의 75%를 제거한다.\n",
    "\n",
    "\n",
    "- Max pooling을 가장 흔하게 사용하며, average pooling과 sum pooling, L2-norm pooling을 사용하기도 한다.\n",
    "- Tensorflow의 pooling layer에서는 `strides=None`의 default 값은 pool_size이다.\n",
    "\n",
    "\n",
    "```python\n",
    "tf.keras.layers.MaxPool1D(\n",
    "    pool_size=2, strides=None, padding='valid',\n",
    "    data_format='channels_last', **kwargs\n",
    ")\n",
    "\n",
    "tf.keras.layers.MaxPool2D(\n",
    "    pool_size=(2, 2), strides=None, padding='valid', data_format=None,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "tf.keras.layers.MaxPool3D(\n",
    "    pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None,\n",
    "    **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "- Tensorflow에서 제공하는 pooing layer\n",
    "\n",
    "```python\n",
    "tf.keras.layers.AveragePooling1D, 2D, 3D\n",
    "tf.keras.layers.GlobalAveragePooling1D, 2D, 3D\n",
    "tf.keras.layers.GlobalMaxPooling1D, 2D, 3D\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pooling vs stride\n",
    "\n",
    "- pooling은 중요성이 떨어지는 pixel을 선택적으로 제거하고 stride는 지정된 위치의 pixel을 제거한다.\n",
    "- 모형의 구조에 따라 효과가 다르므로, 선택이 어려울 때는 두 방법을 모두 시도해 본다. [참고](https://stats.stackexchange.com/questions/387482/pooling-vs-stride-for-downsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fully connected layers (FC)\n",
    "\n",
    "- 위 과정을 필요한 만큼 반복하여 정보량을 줄인 후 이를 하나의 vector로 만들어 classificatin에 사용한다.\n",
    "<br><br>\n",
    " - Flatten layer\n",
    " - Densely connected layers\n",
    " - Dropout layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN의 구성\n",
    "\n",
    "- [CS231 노트](https://cs231n.github.io/convolutional-networks/#architectures) 참고\n",
    "\n",
    "### Layer의 배열\n",
    "\n",
    "- 일반적으로 사용하는 CNN의 구성은 다음과 같다. 보통 $\\mathrm{1\\le N \\le 3, M \\ge 0, 0 \\le K \\le 3}$ 이다.\n",
    "\n",
    "$$\n",
    "\\text{Input} \\rightarrow \n",
    "\\overbrace{\n",
    "\\underbrace{\\mathrm{[ConV \\rightarrow ReLU]}}_{\\times N} \\rightarrow \\mathrm{Pool (optional)}\n",
    "}^{\\times M} \\rightarrow \\text{Flatten} \\rightarrow\n",
    "\\underbrace{\\mathrm{[FC \\rightarrow ReLU]}}_{\\times K} \\rightarrow \\text{FC}$$\n",
    "\n",
    "- pooling layer 이전에 convolutional layer를 충분히 적용해 입력자료의 복잡한 특성을 충분히 잡아내는 것 이 일반적이다. (N = 2, 3)\n",
    "\n",
    "\n",
    "<img src=\"https://www.mdpi.com/sensors/sensors-19-04933/article_deploy/html/images/sensors-19-04933-g001.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel size\n",
    "\n",
    "- 3이나 5정도를 많이 사용하고 7이상은 입력자료를 직접 처리하는 첫 layer에서 주로 사용한다.\n",
    "- padding=same 이 일반적인 padding 방법이다.\n",
    "- 크기가 5인 kernel 하나보다 3인 kernel 둘을 이용하는 것이 더 유리하며, 일반적으로 큰 kernel 보다는 전체 parameter 수가 비슷한 작은 kernel을 다수 사용는 것이 좋을 수 있다고 한다. [참고- VGGNet](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "\n",
    "- 커다란 receptive field 하나 보다는 작은 receptive field 여러 개를 적층하는 것이 선호된다.\n",
    "- kernel_size가 3인 convolutional layer를 3번 쌓으면 두번째 layer에서는 input 기준으로 $5\n",
    "\\times 5$, 세번째 layer 에선 $7\\times 7$ pixel의 정보를 요약할 수 있다. 이때 추정할 모수의 수는 $3\\times (C\\times (3\\times 3\\times C))=27C^2$이다.\n",
    "- 반면 $7\\times 7$ 인 receptive field의 경우 총 $(C\\times (7\\times 7\\times C))=47C^2$개가 된다.\n",
    "- 보통 작은 filter들이 입력의 특징들을 잘 잡아내므로 작은 receptive field를 적층하는 것이 선호된다.\n",
    "- 다만 backpropagation에서 좀 더 많은 메모리를 필요로 한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Conv1D\n",
    "\n",
    "- 1차원 CNN layer의 가장 흔한 예는 시계열 자료이다. 예를 들어 GDP의 변화를 분석할 때 C, I, G, X, M으로 분해하여 사용한다면 자료는 (None, timesteps, 5)가 된다.\n",
    "\n",
    "- 10년 간 5개의 feature의 월별 자료를 사용하여 AR(12) 모형을 추정한다면 CNN을 다음과 같이 구성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"time_series\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_2 (Conv1D)           (None, 109, 1)            61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61\n",
      "Trainable params: 61\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(name='time_series')\n",
    "model.add(Conv1D(1, kernel_size=12, input_shape = (120, 5)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여러 나라에 대한 동일한 자료를 사용한다면 어떤 network이 좋을지 생각해 보자. (과제)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 12, 12, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 12, 12, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               1179776   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3), input_shape=(28,28,1), activation='relu'))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=None))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = 'adam', metrics = 'accuracy')\n",
    "\n",
    "model.summary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digit Image\n",
    "\n",
    "- 2.4장에서 살펴본 digit image로 훈련을 마친 DNN 모형을 위치를 바꾼 image에 적용한 예측결과는 0.75로 training dataset의 0.99에 비해 정확성이 떨어졌다.\n",
    "- ConvNet모형은 이 문제를 효과적으로 해결할 수 있도록 고안이 되었지만 간단한 network으로는 눈에 띄는 효과를 보기 어려운 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature shape:  (1797, 64)\n",
      "target shape:  (1797,)\n",
      "\n",
      "target names:  [0 1 2 3 4 5 6 7 8 9]\n",
      "features: \n",
      " [ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      "\n",
      "size of training sample:  1078\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "print('feature shape: ', digits.data.shape)\n",
    "print('target shape: ', digits.target.shape)\n",
    "print()\n",
    "print('target names: ', digits.target_names)\n",
    "print('features: \\n', digits.data[0,:])\n",
    "print()\n",
    "n_train_samples = int(0.6*len(digits.data))\n",
    "print('size of training sample: ', n_train_samples)\n",
    "\n",
    "x_train, y_train = digits.data[:n_train_samples, :], digits.target[:n_train_samples]\n",
    "x_test, y_test = digits.data[n_train_samples:, :], digits.target[n_train_samples:]\n",
    "\n",
    "x_train = tf.Tensor(x_train.reshape(-1,8,8,1))\n",
    "\n",
    "#digit_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#digit_train.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_7 (Conv2D)           (None, 8, 8, 16)          80        \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 8, 8, 8)           520       \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,730\n",
      "Trainable params: 5,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnn = tf.keras.models.Sequential()\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(2,2), \n",
    "                               padding='same', input_shape=(8,8,1)))\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=8, kernel_size=(2,2), padding='same'))\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "cnn.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
    "\n",
    "print(cnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss = 0.0 , accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "cnn.compile(optimizer='adam', \n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics='accuracy')\n",
    "\n",
    "#cnn_hist = cnn.fit(digit_train, epochs=100, verbose=0)\n",
    "cnn_hist = cnn.fit(x_train, y_train, epochs=50, verbose=0)\n",
    "\n",
    "loss, acc = cnn_hist.history['loss'][-1], cnn_hist.history['accuracy'][-1]\n",
    "print()\n",
    "print('loss =', round(loss, 2), ', accuracy =', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_image(im):\n",
    "        \n",
    "    squared = im.reshape(-1,8,8).copy()\n",
    "    idx = [7]+list(range(7))\n",
    "    \n",
    "    squared[::2] = squared[::2,:,idx]\n",
    "    \n",
    "    return squared.reshape(-1,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 96ms/step - loss: 2.9865 - accuracy: 0.7733\n",
      "loss = 2.99 , accuracy = 0.7732962369918823\n"
     ]
    }
   ],
   "source": [
    "x_test_shifted = shift_image(x_test).reshape(-1,8,8,1)\n",
    "\n",
    "#digit_test = tf.data.Dataset.from_tensor_slices((x_test_shifted, y_test))\n",
    "#loss, acc = cnn.evaluate(digit_test, verbose=1, batch_size=len(x_test))\n",
    "\n",
    "loss, acc = cnn.evaluate(x_test_shifted, y_test, verbose=1, batch_size=len(x_test))\n",
    "\n",
    "print('loss =', round(loss, 2), ', accuracy =', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet, Residual Networks\n",
    "\n",
    "- 적정한 깊이를 찾는 것은 모든 neural network 모형의 공통된 문제이다. 일반적으로 지나치게 깊은 network의 성과가 좋지 않은 것은 함수형태를 복잡하게 만들어 convergence, vanishing gardient, overfitting 등 여러가지 문제를 일으키기 때문이다.\n",
    "- 실제로 지나치게 deep network에선 identity function을 잘 배우지 못한다.\n",
    "- Regulariztion, dropout layer, initializtion, batch normalization 등으로 문제를 완화시킬 수 있지만 본질적인 해결책은 아니다.\n",
    "\n",
    "<img src='residual_block.png' width=50%>\n",
    "\n",
    "- 위 그림에서 network이 실제로 학습해야하는 결과물이 $H(x)(=F(x)+x)$라고 생각해보자.\n",
    "- Output에 $x$를 더해 훈련시키면 실제로 이 network이 학습해야하는 대상은 $F(x)$가 된다.\n",
    "- 만일 해당 network에서 학습해야할 것이 없다면 $F(x)=0$으로 만들어 이 network을 건너뛰게 된다.\n",
    "- $x$를 직접 더해주기 때문에 (skp connection) parameter의 수도 동일하며, $x$의 gradient가 ouput에 미치는 영향력이 줄지 않는다.\n",
    "- Residual Network이라고 부르는 이유는 residual block 안의 layer들은 residual 형태인 $F(x) = H(x) - x$을 학습하기 때문에 붙여진 이름이다.\n",
    "\n",
    "\n",
    "- Residual block으로만 전체 network을 구성한다면 아무리 많은 layer를 쌓아도 input과 output은 직접 연결되어 있으로 layer의 수의 증가에 따른 부작용이 거의 없다.\n",
    "\n",
    "\n",
    "- NesNet 모형은 (ReLU를 activation function으로 사용하는) convolutional layer러만 구성이 되어 있으며 feature map의 크기는 strides=2로 줄이며, 이때 동일한 정보량을 유지하기 위해 filter의 수를 2배로 늘린다.\n",
    "- Filter의 수를 늘릴 때는 skip connection의 입력 크기를 조정하기 위해 padding을 적용하거나 1x1 kernel을 사용하였다.\n",
    "- Residual network은 convolutional layer 2개마다 skip connection을 적용하므로 residual block은 하나의 skip connection과 2개의 convolutional layer로 구성된다.\n",
    "\n",
    "\n",
    "- ResNet의 파생모형은 [이곳](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035)을 참고\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://production-media.paperswithcode.com/methods/Screen_Shot_2020-09-25_at_10.26.40_AM_SAB79fQ.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural network에서 layer block을 구성할 때 `keras.Model`를 상속받아 처리하는 경우가 많다.\n",
    "- `keras.Model`에는 compile, fit, evaluate, save 등의 method가 정의되어 있고, 내부의 layer에 접근이 쉽기 때문에 block 단위의 검사나 작업이 쉽다.\n",
    "\n",
    "\n",
    "- Layer나 Model class를 만들 때는 `__init__ ` 부분에서 입력과 관련이 없는 변수를 생성하고, `build` 부분에선 input_shape과 관련된 변수들을, `call` 부분에 연산 내용을 적어준다. [참고](https://www.tensorflow.org/tutorials/customization/custom_layers#implementing_custom_layers)\n",
    "\n",
    "[tensorflow tutorial](https://www.tensorflow.org/tutorials/customization/custom_layers#models_composing_layers), \n",
    "[참고 블로그](https://d2l.ai/chapter_convolutional-modern/resnet.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "    \n",
    "    \"\"\"\n",
    "    The Residual block of ResNet\n",
    "    https://www.tensorflow.org/tutorials/customization/custom_layers#models_composing_layers\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kernel_size, filters):\n",
    "        super(ResnetIdentityBlock, self).__init__()\n",
    "        #super(ResnetIdentityBlock, self).__init__(name='') # python 2. 표현\n",
    "        filters1, filters2, filters3 = filters\n",
    "        \n",
    "        self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n",
    "        self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n",
    "        self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\n",
    "        self.bn2c = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv2a(input_tensor)\n",
    "        x = self.bn2a(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.conv2b(x)\n",
    "        x = self.bn2b(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.conv2c(x)\n",
    "        x = self.bn2c(x, training=training)\n",
    "        \n",
    "        x += input_tensor\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "block = ResnetIdentityBlock(1, [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_75 (Conv2D)          (None, 8, 8, 16)          80        \n",
      "                                                                 \n",
      " resnet_identity_block_8 (Re  (None, 8, 8, 16)         1776      \n",
      " snetIdentityBlock)                                              \n",
      "                                                                 \n",
      " resnet_identity_block_9 (Re  (None, 8, 8, 16)         1776      \n",
      " snetIdentityBlock)                                              \n",
      "                                                                 \n",
      " resnet_identity_block_10 (R  (None, 8, 8, 16)         1776      \n",
      " esnetIdentityBlock)                                             \n",
      "                                                                 \n",
      " conv2d_85 (Conv2D)          (None, 8, 8, 8)           520       \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,058\n",
      "Trainable params: 10,770\n",
      "Non-trainable params: 288\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "res = tf.keras.models.Sequential(name=\"ResNet\")\n",
    "res.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(2,2), \n",
    "                               padding='same', input_shape=(8,8,1)))\n",
    "res.add(ResnetIdentityBlock((2,2), (16,16,16)))\n",
    "res.add(ResnetIdentityBlock((2,2), (16,16,16)))\n",
    "res.add(ResnetIdentityBlock((2,2), (16,16,16)))\n",
    "res.add(tf.keras.layers.Conv2D(filters=8, kernel_size=(2,2), padding='same'))\n",
    "res.add(tf.keras.layers.Flatten())\n",
    "res.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss = 0.0 , accuracy = 1.0\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 1.6516 - accuracy: 0.7469\n",
      "loss = 1.65 , accuracy = 0.7468706369400024\n"
     ]
    }
   ],
   "source": [
    "res.compile(optimizer='adam', \n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics='accuracy')\n",
    "\n",
    "#res_hist = cnn.fit(digit_train, epochs=100, verbose=0)\n",
    "res_hist = res.fit(x_train, y_train, epochs=50, verbose=0)\n",
    "\n",
    "loss, acc = res_hist.history['loss'][-1], cnn_hist.history['accuracy'][-1]\n",
    "print()\n",
    "print('loss =', round(loss, 2), ', accuracy =', acc)\n",
    "\n",
    "loss, acc = res.evaluate(x_test_shifted, y_test, verbose=1, batch_size=len(x_test))\n",
    "\n",
    "print('loss =', round(loss, 2), ', accuracy =', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transfer learning\n",
    "\n",
    "- 분석 대상에 대한 잘 알려진 모형이 있다면 이를 기반으로 fine tuning을 생각해 본다.\n",
    "- 뒷 부분의 일부 layer 이외에는 모두 nontrainable option을 적용하고, 뒷 단의 layer들로만 훈련한다.\n",
    "\n",
    "https://www.tensorflow.org/guide/keras/transfer_learning#freezing_layers_understanding_the_trainable_attribute"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "datagen.fit(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "흔히 접할 수 있는 3D CNN의 예로는 MRI scan 자료가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.data.go.kr/data/15084084/openapi.do"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "673.6px",
    "left": "24px",
    "top": "111.125px",
    "width": "238.55px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
