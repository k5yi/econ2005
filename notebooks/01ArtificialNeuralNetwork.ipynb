{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "- AI 모형들은 답이 있는 문제에서 답을 빠르고 정확하게 찾는 것이 목적\n",
    "- AI 모형은 pattern을 학습하는데 특화\n",
    "\n",
    "\n",
    "> Dmitry Efimov, who heads the ML centre of excellence at American Express, said the Intel researchers missed out on the preprocessing aspect of neural networks. “From the problems we have solved recently, it’s pretty clear that if you just apply simple normalization to the tabular data and train any neural network, **the decision trees would outperform**. But if you apply more effort to preprocess data and reduce noisy information from the data, **neural networks will outperform**. The main question is how much effort you want to apply,” he explained.  Addressing Efimov’s argument on “the right kind of preprocessing“, Bojan Tunguz,  a Kaggle GM and a well known face in the ML community, said that a ‘highly competent’ data scientist can massage data and take advantage of any algorithm’s unique characteristics. “Heck, I can do it in such a way to get a logistic regression to outperform XGBoost!” said Tunguz. [source](https://analyticsindiamag.com/deep-learning-xgboost-or-both-what-works-best-for-tabular-data/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning vs. Deep Learning\n",
    "\n",
    "\n",
    "Machine learning | Deep learning\n",
    ":--- | :---\n",
    "A subset of AI | A subset of machine learning\n",
    "Can train on smaller data sets | Requires large amounts of data\n",
    "Requires more human intervention to correct and learn | Learns on its own from environment and past mistakes\n",
    "Shorter training and lower accuracy | Longer training and higher accuracy\n",
    "Makes simple, linear correlations | Makes non-linear, complex correlations\n",
    "Can train on a CPU (central processing unit) | Needs a specialized GPU (graphics processing unit) to train\n",
    "\n",
    "[출처](https://www.coursera.org/articles/ai-vs-deep-learning-vs-machine-learning-beginners-guide)\n",
    "\n",
    "\n",
    "\n",
    "- 일부 machine learning 모형과 대부분의 deep learning 모형은 변수들이 어떤 관계가 있는지 명시적으로 고려하지 않는다.\n",
    "- 의사결정을 위한 자료분석에선 machine learning algorithm을 압도적으로 많이 사용 \n",
    "- 경제학에서 deep learning은 의사결정에 필요한 중요한 변수를 생성하는데 많이 사용한다.\n",
    "- Average treatment effect (ATE)와 conditional average treatment effect (CATE)과 같은 treatment effect를 추정하기 위한 double maching learning 이외에도 최근엔 deep learning 모형을 사용하려는 시도가 늘고 있다. [참고](https://www.nature.com/articles/s42256-020-0218-x)\n",
    "\n",
    "ML Algorithm | share of respondents in 2019\n",
    ":-- | --:\n",
    "Linear or Logistic Regression | 69%\n",
    "Decision Trees or Random Forests | 58%\n",
    "Gradient Boosting Machines (xgboost, lightgbm, etc) |38%\n",
    "Convolutional Neural Networks |37%\n",
    "Bayesian Approaches |25%\n",
    "Recurrent Neural Networks |23%\n",
    "Dense Neural Networks |23%\n",
    "Generative Adversarial Networks |7%\n",
    "Transformer Networks |6%\n",
    "Evolutionary Approaches |6%\n",
    "\n",
    "[설문조사 2019](https://www.kdnuggets.com/2019/04/top-data-science-machine-learning-methods-2018-2019.html)\n",
    "[설문조사 2020](https://www.infoq.com/news/2021/02/kaggle-report-2020/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Algorithms\n",
    "\n",
    "- 변수사이 비선형관계를 효과적으로 fitting\n",
    "- Cell 단위의 연산을 직병렬로 제한없이 손쉽게 연결 가능한 구조\n",
    "- 최적화 과정은 전통적인 통계모형과 아주 다르다.\n",
    "- Algorithm을 낮은 비용으로 구현 가능\n",
    "\n",
    "\n",
    "#### (Feed forward) Artificial Neural Network (ANN)\n",
    " - perceptron, multilayer perceptron(MLP), deep neural network(DNN)\n",
    "\n",
    "\n",
    "#### Recurrent Neural Network (RNN)\n",
    " - 자료의 순서 정보를 명시적으로 분석에 이용\n",
    " - Long Short-Term Memory (LSTM) and variants\n",
    " - Attention\n",
    " - transformer, transformer XL\n",
    "\n",
    "\n",
    "#### Convolutional Neural Network (CNN)\n",
    " - Pattern 학습에 효과적\n",
    " - 공간과 시간"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Linear Threshold Unit - an artificial neuron\n",
    "\n",
    "- Linear threshold unit (LTU)는 가장 단순한 형태의 feedforward neural network으로 입력값의 가중합에 heaviside step function을 적용하여 boolean 변수를 출력하는 algorithm이다.\n",
    "\n",
    "<img src=\"https://github.com/k5yi/econ2005/blob/master/images/NNfig1-1LTU.png?raw=True\" width=40%/>\n",
    "\n",
    "- LTU는 logistic regression과 동일한 구조 (사용방법의 차이)\n",
    "\n",
    "\n",
    "- 각 cell의 연산 결과를 최종 결과로 이용하기도 하고<br>\n",
    "다른 (혹은 해당) cell의 정보의 흐름을 통제하는 switch로도 사용한다.\n",
    "\n",
    "- Bias $b$는 중요한 신호의 강도가 약할 때 이를 증폭시키는 역할을 한다.\n",
    "\n",
    "- Activation function은 작은 자극에는 반응을 하지 않고 일정 수준 이상의 자극에만 반응하도록 한다.<br>\n",
    "\n",
    "- Perceptron은 하나의 layer를 구성하는 모든 neuron들이 입력과 연결된 network이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Single-layer) Perceptron 구조\n",
    "\n",
    "- 일반적인 (비선형)mapping에 유용\n",
    "- 실제 응용에선 미분이 가능한 activation function을 사용한다.\n",
    "\n",
    "<img src=\"https://github.com/k5yi/econ2005/blob/master/images/NNfig1-2perceptron.png?raw=True\" width=30%/>\n",
    "\n",
    "1. input layer\n",
    "  - input 을 layer(tf.keras.layers.InputLayer)로 구성할 수도 있지만 보통은 생략하거나 tensor(tf.keras.Input)로 취급 <br>\n",
    "  이러한 이유로 perceptron을 single-layer perceptron이라 부른다.\n",
    "\n",
    "\n",
    "2. output layer\n",
    "\n",
    "$$ \\text{output = activation( dot(input, weights) + bias )}$$\n",
    "\n",
    "- input $x\\in \\mathbb{R}^{n\\times K}$, input.shape = (batch\\_size, n\\_feature) <br>\n",
    "- weight $W \\in \\mathbb{R}^{K \\times m}$, weights.shape = (n\\_features, n\\_units), (input size, output size)<br>\n",
    "- bias $b \\in \\mathbb{R}^{m}$, bias.shape = (n\\_units,)<br>\n",
    "- activation input $z = W \\mathbf{x} +b \\in \\mathbb{R}^{m}$, output.shape = (batch\\_size, n\\_units)<br>\n",
    "- activation function $a:\\mathbb{R}^m \\rightarrow \\mathbb{R}, \\mathbb{R}^C$\n",
    "- activation $z = W \\mathbf{x} +b \\in \\mathbb{R}^{m}$, output.shape = (batch\\_size, n\\_units)<br>\n",
    "  - activation 함수의 함수값은 activation이라고 부른다.\n",
    "  - 종종 activation 함수의 input을 지칭하기도 하지만 activation함수의 input은 보통은 $z$를 사용하여 구분한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron과 Neural Network\n",
    "\n",
    "- 인간의 neuron은 사전에 정의된 기능에 따라 작동하며 perceptron은 인간의 neuron 구조를 모형화한 것이다.\n",
    "- perceptron에 output layer를 추가하여 neural network 모형을 구성한다.\n",
    "- 자료를 이용하여 loss의 크기가 작아지도록 weight와 bias를 조정해 나가는 과정을 학습이라고 한다.\n",
    "- Neural Network은 artificial neuron을 이용하여 입출력에 대해 분석대상의 특성을 모사하도록 학습하는 algorithm이다.\n",
    "\n",
    "\n",
    "- Maching learning vs. Neural Newtwork\n",
    " - epoch나 batch size 등 결과에 영향을 미치는 tuning 모수값을 경험으로 결정해야 한다. <br> 대부분의 모형에서 hyperparameter의 값은 performanace는 큰 영향을 미친다.\n",
    " - tuning parameter 값에 따라 학습시간이 지나치게 길어지거나 수렴하지 않을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single neuron regression\n",
    "\n",
    "- OLS는 Gauss-Markov 가정 하에서 MSE를 가장 작게하는 weight를 closed form으로 계산한다.\n",
    "- Perception을 이용하면 반복적인 근사방법을 지정한 횟수만큼 적용하여 답을 구한다.\n",
    "\n",
    "\n",
    "- 전체 MLP의 neuron을 linear activation 함수로 구성한 모형은 사용하지 않는다.\n",
    " - OLS뿐 아니라 neural network 모형에서도 표본의 전처리는 performance에 큰 영향을 미친다.\n",
    "\n",
    "\n",
    "- Activation function <br>\n",
    "`tf.keras.activations.linear(x)`\n",
    "\n",
    "- Loss function<br>\n",
    "  - Mean Squared Error, `tf.keras.losses.mean_squared_error()`\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{n} \\sum^n_{i=1} \\left(y_i - \\hat{y}_i \\right)^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Perceptron vs. linear regression with a polynomial\n",
    "\n",
    "- advantages over polynomial regression\n",
    " - 추정할 모수의 수가 작다.\n",
    " - 컴퓨터의 성능을 고려하면 현실적인 대안이다.\n",
    "\n",
    "\n",
    "- disadvantages\n",
    " - '목적함수'가 score라면 $\\mathbf{w}$가 유일하게 결정되지 않을 수 있다.\n",
    " - Regression 문제에서도 같은 결과를 얻을 수 있는 weight matrix가 무수히 많이 존재할 수 있다.<br>\n",
    "따라서 개별 feature가 target에 미치는 영향을 정확히 알기 어렵다.\n",
    "\n",
    "\n",
    "\n",
    "[Polynomial Regression As an Alternative to Neural Nets](https://arxiv.org/abs/1806.06850)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single neuron binary classifier\n",
    "\n",
    "- Activation function\n",
    "  - 특별한 이유가 없다면 보통 sigmoid를 사용\n",
    "\n",
    "```python\n",
    "tf.keras.activations.sigmoid(x)\n",
    "```\n",
    "\n",
    "- Loss function\n",
    "  1. Bianry sigmoid classifier with cross-entropy loss (logistic regression model)<br>\n",
    "`tf.keras.losses.BinaryCrossentropy()`\n",
    "<br><br>\n",
    "$$  \\text{loss}(y, \\hat{y}) = −\\left(y \\log(\\hat{y})+(1−y)\\log(1−\\hat{y})\\right), \\quad  y \\in \\{0,1\\}$$<br><br>\n",
    "  2. Binary SVM classifier, Hinge loss<br>\n",
    "SVM에서 사용하는 loss 로 target 값을 $\\{-1,1\\}$로 변환하여 사용한다.<br>\n",
    "`tf.keras.losses.Hinge()`\n",
    "\n",
    "$$ \\text{loss}(y, \\hat{y}) = \\max(0, 1- y \\cdot \\hat{y}), \\quad y\\in \\{-1,1\\}$$\n",
    "\n",
    "[information entropy](https://mlstats.tistory.com/entry/%EC%A0%95%EB%B3%B4%EC%99%80-%EB%B6%84%ED%8F%AC-information-entropy), \n",
    "[infomation gain](https://mlstats.tistory.com/entry/%EC%A0%95%EB%B3%B4%EC%9D%98-%ED%95%9C%EA%B3%84%EC%A0%81%EC%9D%B8-%EA%B0%80%EC%B9%98-information-gain),\n",
    "[cross entropy와 Kullback-Leibler divergence](https://mlstats.tistory.com/entry/%EC%A0%95%EB%B3%B4%EB%9F%89%EC%9D%98-%EC%B0%A8%EC%9D%B4-Cross-entropy-%EC%99%80-Kullback-Leibler-divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Perceptron과 logistic regression\n",
    "\n",
    "- Perceptron의 activation 함수로 sigmoid 함수를 사용하면 logistic regression과 동일한 함수꼴이 된다.\n",
    " - 완벽한 분리가 가능한 경우에는 logistic regression을 사용할 수 없다.\n",
    " - 전처리 과정에 따라 두 모형의 추정결과는 아주 다를 수 있다.\n",
    "\n",
    "\n",
    "- 일반적으로 decision boundary는 유일하게 결정되지 않는다.\n",
    " - 완벽하게 분리되지 않는 경우에도 손실함수에 따라 유일하지 않을 수 있다.\n",
    " - 완전하게 분리가 되지 않는 경우에는 초기값에 따라 다른 결과를 얻는다.<br>\n",
    " - 추정해야할 모수의 수가 표본수 보다 많은 경우가 많다.<br>\n",
    " \n",
    "\n",
    "- Multiclass classfication \n",
    " - Multinomial regression은 binary classfication 추정방법을 사용하며 이들 결과를 OVA나 OVO 로 정리하여 분류에 사용한다. \n",
    " - Neural network에선 softmax 함수를 이용하여 전체 class의 확률을 한번에 계산한다.\n",
    " \n",
    "\n",
    "- Binary classification 문제에서는 neural network 보다는 support vector machine을 많이 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multilayer Perceptron\n",
    "\n",
    "- 비선형문제\n",
    " - Multilayer perceptron은 XOR문제와 같이 비선형 문제를 해결하기 위해 고안 <br>\n",
    " - Decision boundary 혹은 level curve가 feature들의 비선형함수라면 선형관계를 가정한 perceptron으론 해결이 불가능\n",
    "\n",
    "<img src=\"https://github.com/k5yi/econ2005/blob/master/images/NNfig1-3XOR.png?raw=True\" width=\"500\"/>\n",
    "\n",
    "- 이 문제는 perceptron을 적층한 multilayer perceptron으로 해결이 가능하다는 것이 알려졌다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 첫 번째 layer의 선형결합을 다름과 같이 정의한다.\n",
    "\n",
    "$$\\begin{array}{l}\n",
    "z_1(x_1,x_2) = 2 x_1 +  2 x_2 - 1 \\\\\n",
    "z_2(x_1,x_2) = - 2 x_1 - 2 x_2 + 3 \\end{array}$$\n",
    "\n",
    "$$\\begin{array}{llll}\n",
    "o_1(0,0) = g_1(-1)=0, & o_1(0,1) = o_1(1,0) = g_1(1) = 1, &  o_1(1,1) = g_1(3)=1 \\\\\n",
    "o_2(0,0) = g_2(3)=1, & o_2(0,1) = o_2(1,0) = g_2(1) = 1, & o_2(1,1) = g_2(- 1)=0\n",
    "\\end{array}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "- 두 번째 layer의 선형결합을 $g(o_1, o_2) = o_1 + o_2 - 1$으로 정의하면 완벽한 분리가 가능하다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\Pr(y=1) = \n",
    "\\overbrace{\n",
    "\\sigma ( w_{01} \\cdot \\\n",
    "\\underbrace{\\sigma(w_{11} x_1+w_{12} x_2+ b_1)}_{\\text{hidden unit 1}} \n",
    "+ w_{02} \\cdot \n",
    "\\underbrace{\\sigma(w_{21} x_1+w_{22} x_2+ b_2)}_{\\text{hidden unit 2}} \n",
    "+ b_0 )\n",
    "}^{\\text{output layer}}$$\n",
    "\n",
    "<img src = \"https://github.com/k5yi/econ2005/blob/master/images/NNfig1-4xor_nn.png?raw=True\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### MLP와 Deep Neural Network\n",
    "\n",
    "- 다수의 perceptron을 병렬로 연결 <br>\n",
    "\n",
    "1. Multi-Layer Perceptron: input과 output LTU 사이에 **하나 이상**의 LTU layer\n",
    "2. Deep Neural Network: input과 output LTU 사이에 **둘 이상**의 LTU layer\n",
    "3. Dense Neural Network: fully connected (FC) layer<br>\n",
    "각 layer의 모든 모든 neuron 혹은 unit들은 다음 단계 layer의 모든 unit에 연결<br>\n",
    "함수꼴을 (지나치게 일반화시켜) overfitting의 원인이 될 수 있다.\n",
    "\n",
    "\n",
    "- Input layer 이외의 layer에 속한 모든 node는 activation function을 사용하는 artificial neuron\n",
    "- Hidden layer는 input layer와 output layer 사이에서 병렬로 연결된 artificial neuron들의 묶음 \n",
    "\n",
    "<img src=\"https://github.com/k5yi/econ2005/blob/master/images/NNfig1-5networks.png?raw=True\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "- 모형의 유연성만큼 hyperparameter의 수가 많다.\n",
    " - layer의 수, neuron의 수, activation function, initial weight, epoch, batch_size, regularization, ... \n",
    " \n",
    "- 유사한 자료를 유사한 목적으로 분석한다면 사전 훈련된 network을 재활용할 수 있다.\n",
    "\n",
    "- 가능한 조합을 grid search로 비교하기는 너무 많은 자원이 필요하므로 현실적으로 불가능\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layer의 수\n",
    "\n",
    "1. neuron의 수가 충분히 많다면 하나의 hidden layer로 대부분의 복잡한 함수를 근사할 수 있다.\n",
    "2. Hidden layer의 수를 증가시키면 작은 수의 neuron으로도 비슷한 결과를 얻을 수 있다.\n",
    "3. 2개 정도의 hidden layer면 대부분의 자료들를 상당한 정확도로 fit할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron 수\n",
    "\n",
    "- input 'layer'는 전적으로 자료 형태에 의존하고 output layer의 neuron수는 분석 목적에 따라 결정된다.\n",
    "- 보통 hidden layer를 쌓아가면서 neuron의 수를 줄여나간다.\n",
    "- 모든 hidden layer의 neuron수를 같게 하고 neuron의 수를 바꾸는 것도 많이 사용하는 방법이다.\n",
    "- 같은 수의 neuron을 사용한다면 layer당 unit수를 늘리는 것 보다는 layer의 수를 늘리는 것이 더 효과적이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation 함수\n",
    "\n",
    "- ReLU는 대부분의 분석문제에서 hidden layer의 activation function으로 사용할 수 있다.\n",
    "   - 상대적으로 수렴속도가 빠르다.\n",
    "   - sigmoid나 tanh와 달리 satuation이 없다.\n",
    "\n",
    "\n",
    "- Output layer\n",
    "  - multiclass classification: softmax\n",
    "  - binary classification: sigmoid\n",
    "  - regression: linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "1. Atificial Neural Network (ANN) or Neural Network (NN)\n",
    "  - A collection of artificial neurons\n",
    "  - (Single-layer) perceptron은 input과 output layer, 그리고 step function을 activation으로 하는 artificial neuron\n",
    "  - Hidden layer가 포함된 perceptron은 multilayer perceptron (MLP)로 구분\n",
    "\n",
    "\n",
    "2. Deep learning\n",
    "  - Deep Neural Network (DNN)\n",
    "  - Convolutional Neural Network (CNN)\n",
    "  - Recurrent Neural Network (RNN)\n",
    "  - Encoder-decoder model\n",
    "  - Generative Adversarial Network (GAN)\n",
    "\n",
    "\n",
    "3. Machine learning algorithm과의 차이점\n",
    "  - feature selection, extraction은 NN algorithm 내에서 암묵적으로 이루어진다.\n",
    "  - Blackbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch, Tensorflow, Keras, Pytorch lightning\n",
    "\n",
    "- Pytorch와 tensorflow는 numpy의 확장형으로 neural network 연산에 특화된 package\n",
    "\n",
    "$ $ | supported by | advantages\n",
    ":--- |:--- |:--- \n",
    "TensorFlow | Google | visualization, popular, parallel computing \n",
    "Pytorch | Facebook(Meta) | Pythonic, getting popular, flexible, fast\n",
    "\n",
    "- Keras는 tensorflow나 theano, CNTK 등을 backend로 사용자의 편의성을 높여주는 package\n",
    "- **Tensorflow**는 가독성이 떨어지며 debugging이 어렵다고 알려져 있었지만 2.0에서 Keras를 통합\n",
    "  - 대량의 자료에 강점\n",
    "  - `tk.keras` API는 가독성이 좋으며 연구보다는 응용\n",
    "  - functinoal programming으로 numpy와 유사\n",
    "  - [plain tensorflow without keras](https://www.tutorialspoint.com/tensorflow/tensorflow_single_layer_perceptron.htm)\n",
    "  \n",
    "\n",
    "- **Pytorch**는 tensorflow에 비해 code가 상대적으로 간결하며 자연어와 image 자료에 많이 사용\n",
    "  - 속도에 어느 정도 이점이 있고 연구나 심도있는 응용\n",
    "  - Pytorch lightning은 pytorch의 가독성을 높인 version으로 tk.keras에 대응되는 api\n",
    "  - pythonic\n",
    "  - [plain pytorch and pytorch lighting](https://www.machinecurve.com/index.php/2021/01/26/creating-a-multilayer-perceptron-with-pytorch-and-lightning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Algorithm\n",
    "\n",
    "- Deep learning 모형의 설계에는 gradient descent와 관련된 문제들이 많다.\n",
    "- AI 모형의 최적화엔 gradient descent와 Newton's method를 많이 사용\n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "- Gradient vector는 함수값이 가장 빠르게 증가하는 방향을 표시\n",
    "- 극소화 문제에선 gradient와 반대 방향으로 모수를 조정하여 극소점을 찾는다.\n",
    "\n",
    "\n",
    "$$ \\mathbf{x}_{n+1} = \\mathbf{x}_{n} - s \\cdot \\nabla f(\\mathbf{x}_{n}) $$\n",
    "\n",
    "### Newton's method (Newton-Raphson method)\n",
    "\n",
    "- 선형근사방법을 이용하여 방정식의 해를 찾는 방법이다.\n",
    "- 비선형방정식의 해를 구하는 방법\n",
    "r\n",
    "$$ L(\\mathbf{x}_{n+1}) = f(\\mathbf{x}_n) + \\nabla f(\\mathbf{x}_n) (\\mathbf{x}_{n+1}-\\mathbf{x}_n) = 0$$\n",
    "\n",
    "$$ \\mathbf{x}_{n+1} = \\mathbf{x}_n - \\frac{\\nabla f(\\mathbf{x}_n)}{f(\\mathbf{x}_n)}$$\n",
    "\n",
    "- 연속적으로 두번 미분이 가능한 함수의 국지적극점에선 일계도함수의 값이 0이므로, 일계도함수에 직접 적용한다.\n",
    "\n",
    "$$ \\nabla f (\\mathbf{x}_{n+1}) \\approx \\nabla f(\\mathbf{x}_n) + H f(\\mathbf{x}_n) (\\mathbf{x}_{n+1}-\\mathbf{x}_n) = 0$$\n",
    "\n",
    "$$ \\mathbf{x}_{n+1} = \\mathbf{x}_n  - \\left[ H f(\\mathbf{x}_n) \\right]^{-1} \\nabla f(\\mathbf{x}_n)$$\n",
    "\n",
    "- Newton's method는 inflection point가 있거나 국지적 극소점과 극대점이 가까이 있는 경우 시작점에 따라 예상치 못한 결과를 낳기도 한다.\n",
    "- 다음 최적화문제를 풀어보면 Hessian이 양정부호를 갖는지, 음정부호를 갖는지에 따라 해의 해석이 달라지는 것을 볼 수 있다.\n",
    "\n",
    "$$\\min_\\mathbf{h} f(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x} ) \\approx \\nabla f(\\mathbf{x} + \\mathbf{h}) + \\frac{1}{2} \\mathbf{h}^T \\left[ H f(\\mathbf{x}_n) \\right] \\mathbf{h}, \\quad \\mathbf{h} = \\mathbf{x}_{n+1} - \\mathbf{x}_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 참고자료\n",
    "\n",
    "\n",
    "1. [100일 완성 Deep Learning](https://ravimashru.github.io/100-days-of-deep-learning/)\n",
    "\n",
    "2. [Serious introduction and review](https://stanford.edu/~shervine/teaching/)\n",
    "\n",
    "3. [1Blue1Brown series, ](https://youtu.be/aircAruvnKk)\n",
    " - But what is a neural network? | Chapter 1, Deep learning\n",
    " - Gradient descent, how neural networks learn | Chapter 2, Deep learning\n",
    " - What is backpropagation really doing? | Chapter 3, Deep learning\n",
    " - Backpropagation calculus | Chapter 4, Deep learning\n",
    "\n",
    "4. [Neural network 소개 Blog](https://colah.github.io/)<br>\n",
    "기본 개념들과 모형들을 그림으로 잘 설명한다.\n",
    "\n",
    "5. Implementation을 위해서는 간단한 예제로 분석의 기본틀을 이해한 후 공식 document를 참고하는 것을 추천 <br>\n",
    "[Tensorflow 빠르게 시작하기](https://www.tensorflow.org/tutorials/quickstart/beginner)<br>\n",
    "[세세한 가이드](https://www.tensorflow.org/guide)\n",
    "\n",
    "6. [Standford CS230](https://cs230.stanford.edu/), deep learning 강의<br>\n",
    "컴퓨터 공학 전공을 대상으로 한 강의로 대부분 이론적인 내용을 다룬다.<br>\n",
    "강의자료, 동영상을 볼 수 있다. Implementation은 [blog](https://cs230.stanford.edu/blog/) 참고\n",
    "\n",
    "7. [Standford CS231n](http://cs231n.stanford.edu/), convolutional neural network 강의, Vision Recognition<br>\n",
    "[보충자료](https://cs231n.github.io/)<br>\n",
    "[강의 동영상, Youtube](https://www.youtube.com/playlist?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk) \n",
    "\n",
    "8. [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/book/2155)\n",
    "\n",
    "9. [눈으로 확인할 수 있는 parallel computing의 효과](https://youtu.be/-P28LKWTzrI)\n",
    "\n",
    "10. [curriculum learning - Time series 사례](http://infosci.cornell.edu/~koenecke/files/Deep_Learning_for_Time_Series_Tutorial.pdf)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
